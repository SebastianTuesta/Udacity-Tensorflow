{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Other imports for processing data\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus, num_words=-1):\n",
    "  # Fit a Tokenizer on the corpus\n",
    "  if num_words > -1:\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "  else:\n",
    "    tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(corpus)\n",
    "  return tokenizer\n",
    "\n",
    "def create_lyrics_corpus(dataset, field):\n",
    "  # Remove all other punctuation\n",
    "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
    "  # Make it lowercase\n",
    "  dataset[field] = dataset[field].str.lower()\n",
    "  # Make it one long string to split by line\n",
    "  lyrics = dataset[field].str.cat()\n",
    "  corpus = lyrics.split('\\n')\n",
    "  # Remove any trailing whitespace\n",
    "  for l in range(len(corpus)):\n",
    "    corpus[l] = corpus[l].rstrip()\n",
    "  # Remove any empty lines\n",
    "  corpus = [l for l in corpus if l != '']\n",
    "\n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
      "495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-140-fbdddccf8583>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset from csv - just first 10 songs for now\n",
    "dataset = pd.read_csv('tmp/songdata.csv', dtype=str)[:10]\n",
    "# Create the corpus using the 'text' column containing lyrics\n",
    "corpus = create_lyrics_corpus(dataset, 'text')\n",
    "# Tokenize the corpus\n",
    "tokenizer = tokenize_corpus(corpus)\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tsequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences for equal input length \n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
    "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
    "# One-hot encode the labels\n",
    "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "97\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
      "   4]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
      " 287]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Check out how some of our data is being stored\n",
    "# The Tokenizer has just a single index per word\n",
    "print(tokenizer.word_index['know'])\n",
    "print(tokenizer.word_index['feeling'])\n",
    "# Input sequences will have multiple indexes\n",
    "print(input_sequences[5])\n",
    "print(input_sequences[6])\n",
    "# And the one hot labels will be as long as the full spread of tokenized words\n",
    "print(one_hot_labels[5])\n",
    "print(one_hot_labels[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "62/62 [==============================] - 9s 32ms/step - loss: 5.9910 - accuracy: 0.0267\n",
      "Epoch 2/200\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 5.4398 - accuracy: 0.0363 0s - loss: 5.4476 - ac\n",
      "Epoch 3/200\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 5.3717 - accuracy: 0.0429\n",
      "Epoch 4/200\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 5.3218 - accuracy: 0.0409\n",
      "Epoch 5/200\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 5.2573 - accuracy: 0.0358\n",
      "Epoch 6/200\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 5.1908 - accuracy: 0.0409\n",
      "Epoch 7/200\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 5.1273 - accuracy: 0.0489\n",
      "Epoch 8/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 5.0609 - accuracy: 0.0515\n",
      "Epoch 9/200\n",
      "62/62 [==============================] - 1s 16ms/step - loss: 4.9947 - accuracy: 0.0681\n",
      "Epoch 10/200\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 4.9195 - accuracy: 0.0762\n",
      "Epoch 11/200\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 4.8371 - accuracy: 0.0742\n",
      "Epoch 12/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 4.7381 - accuracy: 0.0757\n",
      "Epoch 13/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 4.6364 - accuracy: 0.0898\n",
      "Epoch 14/200\n",
      "62/62 [==============================] - 1s 21ms/step - loss: 4.5380 - accuracy: 0.1024\n",
      "Epoch 15/200\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 4.4394 - accuracy: 0.1160\n",
      "Epoch 16/200\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 4.3425 - accuracy: 0.1201\n",
      "Epoch 17/200\n",
      "62/62 [==============================] - 2s 29ms/step - loss: 4.2523 - accuracy: 0.1443\n",
      "Epoch 18/200\n",
      "62/62 [==============================] - 2s 36ms/step - loss: 4.1505 - accuracy: 0.1549 1s - l - ETA: 0s - loss: 4\n",
      "Epoch 19/200\n",
      "62/62 [==============================] - 3s 40ms/step - loss: 4.0528 - accuracy: 0.1620\n",
      "Epoch 20/200\n",
      "62/62 [==============================] - 3s 50ms/step - loss: 3.9650 - accuracy: 0.1842\n",
      "Epoch 21/200\n",
      "62/62 [==============================] - 2s 38ms/step - loss: 3.8744 - accuracy: 0.2059\n",
      "Epoch 22/200\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 3.8072 - accuracy: 0.2164\n",
      "Epoch 23/200\n",
      "62/62 [==============================] - 1s 21ms/step - loss: 3.7324 - accuracy: 0.2452 0s - loss: 3.7348 - accuracy: 0.24\n",
      "Epoch 24/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 3.6440 - accuracy: 0.2614\n",
      "Epoch 25/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 3.5697 - accuracy: 0.2820\n",
      "Epoch 26/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 3.4814 - accuracy: 0.2947\n",
      "Epoch 27/200\n",
      "62/62 [==============================] - ETA: 0s - loss: 3.4001 - accuracy: 0.31 - 1s 16ms/step - loss: 3.4086 - accuracy: 0.3133\n",
      "Epoch 28/200\n",
      "62/62 [==============================] - 1s 16ms/step - loss: 3.3451 - accuracy: 0.3214\n",
      "Epoch 29/200\n",
      "62/62 [==============================] - 2s 35ms/step - loss: 3.2797 - accuracy: 0.3446\n",
      "Epoch 30/200\n",
      "62/62 [==============================] - 2s 31ms/step - loss: 3.2322 - accuracy: 0.3471\n",
      "Epoch 31/200\n",
      "62/62 [==============================] - 2s 37ms/step - loss: 3.1654 - accuracy: 0.3532\n",
      "Epoch 32/200\n",
      "62/62 [==============================] - 2s 36ms/step - loss: 3.0906 - accuracy: 0.3784\n",
      "Epoch 33/200\n",
      "62/62 [==============================] - 2s 29ms/step - loss: 3.0335 - accuracy: 0.3850\n",
      "Epoch 34/200\n",
      "62/62 [==============================] - 2s 34ms/step - loss: 2.9760 - accuracy: 0.3900\n",
      "Epoch 35/200\n",
      "62/62 [==============================] - 2s 37ms/step - loss: 2.9191 - accuracy: 0.4051\n",
      "Epoch 36/200\n",
      "62/62 [==============================] - 2s 37ms/step - loss: 2.8637 - accuracy: 0.4173\n",
      "Epoch 37/200\n",
      "62/62 [==============================] - 1s 22ms/step - loss: 2.8055 - accuracy: 0.4319\n",
      "Epoch 38/200\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 2.7528 - accuracy: 0.4445 \n",
      "Epoch 39/200\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 2.7014 - accuracy: 0.4546\n",
      "Epoch 40/200\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 2.6434 - accuracy: 0.4632\n",
      "Epoch 41/200\n",
      "62/62 [==============================] - 1s 20ms/step - loss: 2.6002 - accuracy: 0.4677\n",
      "Epoch 42/200\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 2.5600 - accuracy: 0.4758\n",
      "Epoch 43/200\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 2.5516 - accuracy: 0.4738\n",
      "Epoch 44/200\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 2.4641 - accuracy: 0.5005\n",
      "Epoch 45/200\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 2.4232 - accuracy: 0.5020\n",
      "Epoch 46/200\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 2.3760 - accuracy: 0.5187\n",
      "Epoch 47/200\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 2.3348 - accuracy: 0.5368\n",
      "Epoch 48/200\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 2.2899 - accuracy: 0.5338\n",
      "Epoch 49/200\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 2.2332 - accuracy: 0.5555\n",
      "Epoch 50/200\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 2.1918 - accuracy: 0.5631\n",
      "Epoch 51/200\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 2.1598 - accuracy: 0.5807\n",
      "Epoch 52/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.1193 - accuracy: 0.5807\n",
      "Epoch 53/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0766 - accuracy: 0.5918\n",
      "Epoch 54/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.0451 - accuracy: 0.5959\n",
      "Epoch 55/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.0207 - accuracy: 0.6014\n",
      "Epoch 56/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 1.9989 - accuracy: 0.6049\n",
      "Epoch 57/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9451 - accuracy: 0.6155\n",
      "Epoch 58/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.9178 - accuracy: 0.6186\n",
      "Epoch 59/200\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 1.8744 - accuracy: 0.6322\n",
      "Epoch 60/200\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 1.8358 - accuracy: 0.6438\n",
      "Epoch 61/200\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 1.8053 - accuracy: 0.6519\n",
      "Epoch 62/200\n",
      "62/62 [==============================] - 2s 29ms/step - loss: 1.7757 - accuracy: 0.6554 0s - loss: 1.748\n",
      "Epoch 63/200\n",
      "62/62 [==============================] - 2s 36ms/step - loss: 1.7783 - accuracy: 0.6463\n",
      "Epoch 64/200\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 1.7359 - accuracy: 0.6609\n",
      "Epoch 65/200\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 1.6995 - accuracy: 0.6715\n",
      "Epoch 66/200\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 1.7357 - accuracy: 0.6564\n",
      "Epoch 67/200\n",
      "62/62 [==============================] - 2s 30ms/step - loss: 1.6677 - accuracy: 0.6690\n",
      "Epoch 68/200\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 1.6487 - accuracy: 0.6710\n",
      "Epoch 69/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.6351 - accuracy: 0.6786\n",
      "Epoch 70/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.5936 - accuracy: 0.6857\n",
      "Epoch 71/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 1.5307 - accuracy: 0.7038\n",
      "Epoch 72/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.4949 - accuracy: 0.7104\n",
      "Epoch 73/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.4766 - accuracy: 0.7170\n",
      "Epoch 74/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 1.4333 - accuracy: 0.7296\n",
      "Epoch 75/200\n",
      "62/62 [==============================] - 2s 33ms/step - loss: 1.4083 - accuracy: 0.7336\n",
      "Epoch 76/200\n",
      "62/62 [==============================] - 3s 50ms/step - loss: 1.4176 - accuracy: 0.7255\n",
      "Epoch 77/200\n",
      "62/62 [==============================] - 3s 45ms/step - loss: 1.3966 - accuracy: 0.7296\n",
      "Epoch 78/200\n",
      "62/62 [==============================] - 2s 35ms/step - loss: 1.3645 - accuracy: 0.7321\n",
      "Epoch 79/200\n",
      "62/62 [==============================] - 2s 32ms/step - loss: 1.3362 - accuracy: 0.7462\n",
      "Epoch 80/200\n",
      "62/62 [==============================] - 2s 30ms/step - loss: 1.3639 - accuracy: 0.7412\n",
      "Epoch 81/200\n",
      "62/62 [==============================] - 2s 35ms/step - loss: 1.3741 - accuracy: 0.7250\n",
      "Epoch 82/200\n",
      "62/62 [==============================] - 1s 20ms/step - loss: 1.3071 - accuracy: 0.7432\n",
      "Epoch 83/200\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 1.2563 - accuracy: 0.7619\n",
      "Epoch 84/200\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 1.2208 - accuracy: 0.7709\n",
      "Epoch 85/200\n",
      "62/62 [==============================] - 1s 21ms/step - loss: 1.1951 - accuracy: 0.7760\n",
      "Epoch 86/200\n",
      "62/62 [==============================] - 1s 23ms/step - loss: 1.2361 - accuracy: 0.7568\n",
      "Epoch 87/200\n",
      "62/62 [==============================] - 2s 39ms/step - loss: 1.2323 - accuracy: 0.7558\n",
      "Epoch 88/200\n",
      "62/62 [==============================] - 2s 39ms/step - loss: 1.2402 - accuracy: 0.7518\n",
      "Epoch 89/200\n",
      "62/62 [==============================] - 2s 34ms/step - loss: 1.1794 - accuracy: 0.7689\n",
      "Epoch 90/200\n",
      "62/62 [==============================] - 2s 31ms/step - loss: 1.1301 - accuracy: 0.7810\n",
      "Epoch 91/200\n",
      "62/62 [==============================] - 2s 31ms/step - loss: 1.1168 - accuracy: 0.7805\n",
      "Epoch 92/200\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 1.1086 - accuracy: 0.7851\n",
      "Epoch 93/200\n",
      "62/62 [==============================] - 2s 36ms/step - loss: 1.0671 - accuracy: 0.7906 0s - l\n",
      "Epoch 94/200\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 1.0705 - accuracy: 0.7866\n",
      "Epoch 95/200\n",
      "62/62 [==============================] - 2s 29ms/step - loss: 1.0598 - accuracy: 0.7886 0s - loss: 1.0\n",
      "Epoch 96/200\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 1.0283 - accuracy: 0.7977\n",
      "Epoch 97/200\n",
      "62/62 [==============================] - 1s 16ms/step - loss: 1.0238 - accuracy: 0.8017\n",
      "Epoch 98/200\n",
      "62/62 [==============================] - 1s 16ms/step - loss: 1.0130 - accuracy: 0.7921\n",
      "Epoch 99/200\n",
      "62/62 [==============================] - 1s 20ms/step - loss: 0.9895 - accuracy: 0.8047\n",
      "Epoch 100/200\n",
      "62/62 [==============================] - 1s 16ms/step - loss: 0.9693 - accuracy: 0.8073\n",
      "Epoch 101/200\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 0.9466 - accuracy: 0.8148\n",
      "Epoch 102/200\n",
      "62/62 [==============================] - 2s 29ms/step - loss: 0.9288 - accuracy: 0.8219\n",
      "Epoch 103/200\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.9555 - accuracy: 0.8143\n",
      "Epoch 104/200\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.9343 - accuracy: 0.8209 0s - loss: 0\n",
      "Epoch 105/200\n",
      "62/62 [==============================] - 2s 30ms/step - loss: 0.9043 - accuracy: 0.8224\n",
      "Epoch 106/200\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 0.8960 - accuracy: 0.8163\n",
      "Epoch 107/200\n",
      "62/62 [==============================] - 2s 33ms/step - loss: 0.8779 - accuracy: 0.8209 1s - loss: 0.893 - ETA: 0s - loss: 0\n",
      "Epoch 108/200\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 0.8620 - accuracy: 0.8254\n",
      "Epoch 109/200\n",
      "62/62 [==============================] - 2s 31ms/step - loss: 0.8411 - accuracy: 0.8320\n",
      "Epoch 110/200\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 0.8267 - accuracy: 0.8340\n",
      "Epoch 111/200\n",
      "62/62 [==============================] - 1s 22ms/step - loss: 0.8161 - accuracy: 0.8401\n",
      "Epoch 112/200\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 0.8010 - accuracy: 0.8411\n",
      "Epoch 113/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 0.7875 - accuracy: 0.8431\n",
      "Epoch 114/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.7788 - accuracy: 0.8461\n",
      "Epoch 115/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.7735 - accuracy: 0.8486\n",
      "Epoch 116/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.7623 - accuracy: 0.8502\n",
      "Epoch 117/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.7493 - accuracy: 0.8446\n",
      "Epoch 118/200\n",
      "62/62 [==============================] - 1s 21ms/step - loss: 0.7491 - accuracy: 0.8567\n",
      "Epoch 119/200\n",
      "62/62 [==============================] - 1s 22ms/step - loss: 0.7574 - accuracy: 0.8451\n",
      "Epoch 120/200\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.7401 - accuracy: 0.8496\n",
      "Epoch 121/200\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.7333 - accuracy: 0.8557\n",
      "Epoch 122/200\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 0.7264 - accuracy: 0.8491\n",
      "Epoch 123/200\n",
      "62/62 [==============================] - 2s 24ms/step - loss: 0.7138 - accuracy: 0.8572\n",
      "Epoch 124/200\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.7004 - accuracy: 0.8592\n",
      "Epoch 125/200\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.7064 - accuracy: 0.8572\n",
      "Epoch 126/200\n",
      "62/62 [==============================] - 2s 30ms/step - loss: 0.6876 - accuracy: 0.8602\n",
      "Epoch 127/200\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 0.6705 - accuracy: 0.8592\n",
      "Epoch 128/200\n",
      "62/62 [==============================] - 1s 20ms/step - loss: 0.6660 - accuracy: 0.8653\n",
      "Epoch 129/200\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 0.6520 - accuracy: 0.8638\n",
      "Epoch 130/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.6528 - accuracy: 0.8633\n",
      "Epoch 131/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.6473 - accuracy: 0.8623\n",
      "Epoch 132/200\n",
      "62/62 [==============================] - 1s 16ms/step - loss: 0.6395 - accuracy: 0.8643\n",
      "Epoch 133/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 0.6498 - accuracy: 0.8607\n",
      "Epoch 134/200\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 0.6516 - accuracy: 0.8577\n",
      "Epoch 135/200\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 0.6307 - accuracy: 0.8618\n",
      "Epoch 136/200\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 0.6152 - accuracy: 0.8729\n",
      "Epoch 137/200\n",
      "62/62 [==============================] - 2s 34ms/step - loss: 0.6075 - accuracy: 0.8678\n",
      "Epoch 138/200\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.6005 - accuracy: 0.8688\n",
      "Epoch 139/200\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 0.5854 - accuracy: 0.8769\n",
      "Epoch 140/200\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.6373 - accuracy: 0.8607\n",
      "Epoch 141/200\n",
      "62/62 [==============================] - 2s 26ms/step - loss: 0.6476 - accuracy: 0.8532\n",
      "Epoch 142/200\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 0.6219 - accuracy: 0.8648\n",
      "Epoch 143/200\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 0.6536 - accuracy: 0.8502\n",
      "Epoch 144/200\n",
      "62/62 [==============================] - 1s 21ms/step - loss: 0.6167 - accuracy: 0.8643\n",
      "Epoch 145/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.5981 - accuracy: 0.8648\n",
      "Epoch 146/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.6114 - accuracy: 0.8597\n",
      "Epoch 147/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 0.5842 - accuracy: 0.8683\n",
      "Epoch 148/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 0.5579 - accuracy: 0.8784\n",
      "Epoch 149/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 0.5496 - accuracy: 0.8804\n",
      "Epoch 150/200\n",
      "62/62 [==============================] - 1s 20ms/step - loss: 0.5407 - accuracy: 0.8809\n",
      "Epoch 151/200\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.5305 - accuracy: 0.8829\n",
      "Epoch 152/200\n",
      "62/62 [==============================] - 2s 29ms/step - loss: 0.5253 - accuracy: 0.8824 0s - loss: 0.5263 - accura\n",
      "Epoch 153/200\n",
      "62/62 [==============================] - 1s 24ms/step - loss: 0.5189 - accuracy: 0.8845\n",
      "Epoch 154/200\n",
      "62/62 [==============================] - 2s 38ms/step - loss: 0.5126 - accuracy: 0.8840\n",
      "Epoch 155/200\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 0.5145 - accuracy: 0.8814\n",
      "Epoch 156/200\n",
      "62/62 [==============================] - 2s 31ms/step - loss: 0.5063 - accuracy: 0.8845\n",
      "Epoch 157/200\n",
      "62/62 [==============================] - 2s 31ms/step - loss: 0.4976 - accuracy: 0.8900\n",
      "Epoch 158/200\n",
      "62/62 [==============================] - 2s 31ms/step - loss: 0.5216 - accuracy: 0.8784\n",
      "Epoch 159/200\n",
      "62/62 [==============================] - 1s 20ms/step - loss: 0.6261 - accuracy: 0.8512 0s - loss: 0.6507 - accu\n",
      "Epoch 160/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.5518 - accuracy: 0.8698\n",
      "Epoch 161/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 0.5507 - accuracy: 0.8693\n",
      "Epoch 162/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.5386 - accuracy: 0.8729\n",
      "Epoch 163/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.5449 - accuracy: 0.8673\n",
      "Epoch 164/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.6216 - accuracy: 0.8481\n",
      "Epoch 165/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.5588 - accuracy: 0.8673\n",
      "Epoch 166/200\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.5385 - accuracy: 0.8774\n",
      "Epoch 167/200\n",
      "62/62 [==============================] - 2s 36ms/step - loss: 0.5241 - accuracy: 0.8693\n",
      "Epoch 168/200\n",
      "62/62 [==============================] - 2s 30ms/step - loss: 0.5047 - accuracy: 0.8713 0s - loss: 0.5014 - accuracy\n",
      "Epoch 169/200\n",
      "62/62 [==============================] - 2s 31ms/step - loss: 0.4930 - accuracy: 0.8789\n",
      "Epoch 170/200\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 0.4832 - accuracy: 0.8835 \n",
      "Epoch 171/200\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 0.4696 - accuracy: 0.8885\n",
      "Epoch 172/200\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 0.4645 - accuracy: 0.8875\n",
      "Epoch 173/200\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 0.4549 - accuracy: 0.8890\n",
      "Epoch 174/200\n",
      "62/62 [==============================] - 2s 32ms/step - loss: 0.4469 - accuracy: 0.8905 0s - loss: 0.4406 - accu\n",
      "Epoch 175/200\n",
      "62/62 [==============================] - 1s 20ms/step - loss: 0.4401 - accuracy: 0.8910\n",
      "Epoch 176/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.4622 - accuracy: 0.8819\n",
      "Epoch 177/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4587 - accuracy: 0.8809\n",
      "Epoch 178/200\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 0.4380 - accuracy: 0.8895\n",
      "Epoch 179/200\n",
      "62/62 [==============================] - 1s 16ms/step - loss: 0.4291 - accuracy: 0.8961\n",
      "Epoch 180/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.4211 - accuracy: 0.8915\n",
      "Epoch 181/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.4239 - accuracy: 0.8920\n",
      "Epoch 182/200\n",
      "62/62 [==============================] - 1s 16ms/step - loss: 0.4190 - accuracy: 0.8956\n",
      "Epoch 183/200\n",
      "62/62 [==============================] - 2s 31ms/step - loss: 0.4120 - accuracy: 0.8925\n",
      "Epoch 184/200\n",
      "62/62 [==============================] - 2s 24ms/step - loss: 0.4043 - accuracy: 0.8915\n",
      "Epoch 185/200\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 0.3999 - accuracy: 0.8961\n",
      "Epoch 186/200\n",
      "62/62 [==============================] - 2s 25ms/step - loss: 0.3973 - accuracy: 0.9001\n",
      "Epoch 187/200\n",
      "62/62 [==============================] - 2s 37ms/step - loss: 0.3938 - accuracy: 0.8976\n",
      "Epoch 188/200\n",
      "62/62 [==============================] - 2s 32ms/step - loss: 0.3921 - accuracy: 0.8996\n",
      "Epoch 189/200\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 0.3885 - accuracy: 0.8935 1s - l\n",
      "Epoch 190/200\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 0.3854 - accuracy: 0.9021\n",
      "Epoch 191/200\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 0.3837 - accuracy: 0.9001\n",
      "Epoch 192/200\n",
      "62/62 [==============================] - 2s 30ms/step - loss: 0.3780 - accuracy: 0.9001\n",
      "Epoch 193/200\n",
      "62/62 [==============================] - 1s 18ms/step - loss: 0.3980 - accuracy: 0.8946\n",
      "Epoch 194/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.3919 - accuracy: 0.8920\n",
      "Epoch 195/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 0.3961 - accuracy: 0.8925\n",
      "Epoch 196/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.4015 - accuracy: 0.8910\n",
      "Epoch 197/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4857 - accuracy: 0.8739\n",
      "Epoch 198/200\n",
      "62/62 [==============================] - 1s 16ms/step - loss: 0.4267 - accuracy: 0.8900\n",
      "Epoch 199/200\n",
      "62/62 [==============================] - 1s 19ms/step - loss: 0.3944 - accuracy: 0.8946\n",
      "Epoch 200/200\n",
      "62/62 [==============================] - 2s 27ms/step - loss: 0.3824 - accuracy: 0.8971\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(20)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.show()\n",
    "\n",
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im feeling chills you tell and i never know what in on again come on it way that that over love love as love what as a lousy butterflies give quiet too fate sure love cry what now i am your song care cry as too meant chiquitita song weave chiquitita chiquitita am easy must realized pain here one weave figure am soft love now love hes again time again sound deny but but again again strong on must gone now i mistake found realized deny love again what yourself in could i am as love no love now again again again i\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"im feeling chills\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\t# predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "\tpredicted_probs = model.predict(token_list, verbose=0)[0]\n",
    "\tpredicted = np.random.choice([x for x in range(len(predicted_probs))], p=predicted_probs)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58d76dfd65793fccacb87489547271a4dff26830b1ffc4b526118943a22c8762"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
